{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_data_dir = '/content/drive/MyDrive/딥러닝/Colab Notebook/train'\n",
        "categories = ['adidas', 'converse', 'nike']\n",
        "num_classes = len(categories)\n",
        "\n",
        "#이미지 크기\n",
        "image_w = 240\n",
        "image_h = 240\n",
        "pixels = image_h*image_w*3\n",
        "\n",
        "#trainset원핫코딩\n",
        "X1 = []\n",
        "y1 = []\n",
        "for idx, category in enumerate(categories):\n",
        "  label = [0 for i in range(num_classes)]\n",
        "  label[idx] = 1\n",
        "\n",
        "  train_image_dir = train_data_dir + '/' + category\n",
        "  files = glob.glob(train_image_dir + '/*.jpg')\n",
        "  print(category, \"파일 길이:\", len(files))\n",
        "\n",
        "  for i,f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "\n",
        "    X1.append(data)\n",
        "    y1.append(label)\n",
        "  \n",
        "X1 = np.array(X1)\n",
        "y1 = np.array(y1)\n",
        "\n",
        "test_data_dir = '/content/drive/MyDrive/딥러닝/Colab Notebook/test'\n",
        "\n",
        "#testset원핫코딩\n",
        "X2 = []\n",
        "y2 = []\n",
        "for idx, category in enumerate(categories):\n",
        "  label = [0 for i in range(num_classes)]\n",
        "  label[idx] = 1\n",
        "\n",
        "  test_image_dir = test_data_dir + '/' + category\n",
        "  files = glob.glob(test_image_dir + '/*.jpg')\n",
        "  print(category, \"파일 길이:\", len(files))\n",
        "\n",
        "  for i,f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "\n",
        "    X2.append(data)\n",
        "    y2.append(label)\n",
        "  \n",
        "X2 = np.array(X2)\n",
        "y2 = np.array(y2)\n",
        "\n",
        "X_train = X1\n",
        "y_train = y1\n",
        "X_test = X2\n",
        "y_test = y2\n",
        "\n",
        "X_train = X_train.astype('float64')\n",
        "X_train = X_train/255\n",
        "X_test = X_test.astype('float64')\n",
        "X_test = X_test/255\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, kernel_size = (3,3), input_shape = (240, 240, 3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(3, activation = 'softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "modelpath = '/content/drive/MyDrive/딥러닝/모델저장용'\n",
        "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
        "history = model.fit(X_train, y_train, validation_split = 0.25, epochs = 1000, batch_size = 100, verbose = 0, callbacks = [early_stopping_callback, checkpointer])\n",
        "print(\"\\n Test accuracy: {}.\".format(model.evaluate(X_test)[1]))\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "cf_matrix = confusion_matrix(y_test, pred, normalize='true')\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cf_matrix, annot=False, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)),cbar=False)\n",
        "plt.title('Normalized Confusion Matrix', fontsize = 23)\n",
        "plt.xticks(fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "train_adidas_data = Path('/content/drive/MyDrive/딥러닝/Colab Notebook/train/adidas')\n",
        "filepaths1 = list(train_adidas_data.glob(r'**/*.jpg'))\n",
        "train_converse_data = Path('/content/drive/MyDrive/딥러닝/Colab Notebook/train/converse')\n",
        "filepaths2 = list(train_converse_data.glob(r'**/*.jpg'))\n",
        "train_nike_data = Path('/content/drive/MyDrive/딥러닝/Colab Notebook/train/nike')\n",
        "filepaths3 = list(train_nike_data.glob(r'**/*.jpg'))\n",
        "\n",
        "test_adidas_data = Path('/content/drive/MyDrive/딥러닝/Colab Notebook/test/adidas')\n",
        "filepaths4 = list(test_adidas_data.glob(r'**/*.jpg'))\n",
        "test_converse_data = Path('/content/drive/MyDrive/딥러닝/Colab Notebook/test/converse')\n",
        "filepaths5 = list(test_converse_data.glob(r'**/*.jpg'))\n",
        "test_nike_data = Path('/content/drive/MyDrive/딥러닝/Colab Notebook/test/nike')\n",
        "filepaths6 = list(test_nike_data.glob(r'**/*.jpg'))\n",
        "\n",
        "def proc_img(filepath):\n",
        "    \"\"\"\n",
        "   \t\t이미지데이터의 경로와 label데이터로 데이터프레임 만들기 \n",
        "    \"\"\"\n",
        "\n",
        "    labels = [str(filepath[i]).split(\"/\")[-2] \\\n",
        "              for i in range(len(filepath))]\n",
        "\n",
        "    filepath = pd.Series(filepath, name='Filepath').astype(str)\n",
        "    labels = pd.Series(labels, name='Label')\n",
        "\n",
        "    # 경로와 라벨 concatenate\n",
        "    df = pd.concat([filepath, labels], axis=1)\n",
        "\n",
        "    # index 재설정\n",
        "    df = df.sample(frac=1,random_state=0).reset_index(drop = True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "train_df_adidas = proc_img(filepaths1)\n",
        "train_df_converse = proc_img(filepaths2)\n",
        "train_df_nike = proc_img(filepaths3)\n",
        "\n",
        "test_df_adidas = proc_img(filepaths4)\n",
        "test_df_converse = proc_img(filepaths5)\n",
        "test_df_nike = proc_img(filepaths6)\n",
        "\n",
        "train_df = pd.concat([train_df_adidas,train_df_converse,train_df_nike], ignore_index=True)\n",
        "test_df = pd.concat([test_df_adidas,test_df_converse,test_df_nike], ignore_index=True)\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   validation_split=0.2)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory('/content/drive/MyDrive/딥러닝/Colab Notebook/train',\n",
        "                                                 target_size = (150, 150),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',subset='training')\n",
        "val_gen  = train_datagen.flow_from_directory('/content/drive/MyDrive/딥러닝/Colab Notebook/train',\n",
        "                                                 target_size = (150, 150),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',subset='validation')\n",
        "# Initialising the CNN\n",
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[150, 150, 3]))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Adding convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Step 4 - Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Step 5 - Output Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=8, activation='softmax'))\n",
        "\n",
        "# Compiling the CNN\n",
        "cnn.compile(optimizer = 'adam', \n",
        "            loss = 'categorical_crossentropy', \n",
        "            metrics = ['accuracy'])\n",
        "cnn.summary()\n",
        "\n",
        "cnn.fit(x = train_gen, validation_data = val_gen, epochs = 10)\n",
        "'''"
      ],
      "metadata": {
        "id": "e6tvV7iJfR0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "VglSuSB-BmsI"
      }
    }
  ]
}